= pyTigerGraph GDS Metrics.
Utility for gathering metrics for GNN predictions.
== Base Metric Accumulator.

Usage:
- Call the update function to add a value.
- Get running average by accessing the mean proporty, running sum by the total property, and
number of values by the count property

=== Constructor
`__init__() -> None`

Initialize the accumulator.


=== update
`update(value: float, count: int = 1) -> None`

Add a value to the running sum.

[discrete]
==== **Parameters:**
value (float): 
The value to be added.
count (int, optional): 
The input value is by default treated as a single value.
If it is a sum of multiple values, the number of values can be specified by this
length argument, so that the running average can be calculated correctly. Defaults to 1.


=== mean
`mean() -> float`

Get running average.


=== total
`total() -> float`

Get running sum.


=== count
`count() -> int`

Get running count


== Accuracy Metric.

Accuracy = sum(preds == labels) / len(labels)

Usage:
- Call the update function to add predictions and labels.
- Get accuracy score at any point by accessing the value proporty

=== update
`update(preds: ndarray, labels: ndarray) -> None`

Add predictions and labels to be compared.

[discrete]
==== **Parameters:**
preds (ndarray): 
Array of predicted labels.
labels (ndarray): 
Array of true labels.


=== value
`value() -> float`

Get accuracy score.
[discrete]
==== **Returns:**
Accuracy score (float).


== Recall Metric.

Recall = sum(preds * labels) / sum(labels)

Usage:
- Call the update function to add predictions and labels.
- Get recall score at any point by accessing the value proporty

=== update
`update(preds: ndarray, labels: ndarray) -> None`

Add predictions and labels to be compared.

[discrete]
==== **Parameters:**
preds (ndarray): 
Array of predicted labels.
labels (ndarray): 
Array of true labels.


=== value
`value() -> float`

Get recall score.
[discrete]
==== **Returns:**
Recall score (float).


== Precision Metric.

Precision = sum(preds * labels) / sum(preds)

Usage:
- Call the update function to add predictions and labels.
- Get precision score at any point by accessing the value proporty

=== update
`update(preds: ndarray, labels: ndarray) -> None`

Add predictions and labels to be compared.

[discrete]
==== **Parameters:**
preds (ndarray): 
Array of predicted labels.
labels (ndarray): 
Array of true labels.


=== value
`value() -> float`

Get precision score.
[discrete]
==== **Returns:**
Precision score (float).


