= pyTigerGraph GDS Metrics

:stem: latexmath

Utility for gathering metrics for GNN predictions.

== Accuracy

Accuracy = sum(preds == labels) / len(labels)

[discrete]
==== Usage:

* Call the update function to add predictions and labels.
* Get accuracy score at any point by accessing the value property.


=== update()
`update(preds: ndarray, labels: ndarray) -> None`

Add predictions and labels to be compared.

[discrete]
==== Parameters:
preds (ndarray): 
Array of predicted labels.
labels (ndarray): 
Array of true labels.


=== value()
`value() -> float`

Get accuracy score.
[discrete]
==== Returns:
Accuracy score (float).


== BinaryRecall

Recall = stem:[rac{\sum(preds * labels)}{\sum(labels)}]

This metric is for binary classifications, i.e., both preds and labels are arrays of 0's and 1's.

[discrete]
==== Usage:

* Call the update function to add predictions and labels.
* Get recall score at any point by accessing the value property.


=== update()
`update(preds: ndarray, labels: ndarray) -> None`

Add predictions and labels to be compared.

[discrete]
==== Parameters:
preds (ndarray): 
Array of predicted labels.
labels (ndarray): 
Array of true labels.


=== value()
`value() -> float`

Get recall score.
[discrete]
==== Returns:
Recall score (float).


== BinaryPrecision

Precision = stem:[rac{\sum(preds * labels)}{\sum(preds)}]

This metric is for binary classifications, i.e., both preds and labels are arrays of 0's and 1's.

[discrete]
==== Usage:

* Call the update function to add predictions and labels.
* Get precision score at any point by accessing the value property.


=== update()
`update(preds: ndarray, labels: ndarray) -> None`

Add predictions and labels to be compared.

[discrete]
==== Parameters:
preds (ndarray): 
Array of predicted labels.
labels (ndarray): 
Array of true labels.


=== value()
`value() -> float`

Get precision score.
[discrete]
==== Returns:
Precision score (float).


